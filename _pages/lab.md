---
layout: archive
title: "Research Lab"
permalink: /lab/
author_profile: true
---

{% include base_path %}

### Research Focus

My research primarily focuses on building efficient Machine Learning systems. I specialize in developing innovative methods for neural architecture design, model compression, and automated ML systems that can operate effectively under resource constraints.

My work typically falls into these key areas:

1. **Neural Architecture Search (NAS)**: I develop methods that can automatically discover optimal neural network architectures for specific tasks, with a focus on efficiency and performance.

2. **Model Compression and Pruning**: My research explores novel techniques to reduce the computational requirements of deep learning models while maintaining accuracy, particularly focusing on pruning at initialization.

3. **Efficient Deep Learning**: I work on creating algorithms that significantly reduce ML computation across the full lifecycle from architecture discovery to training and inference.

4. **Controllable Generation with Diffusion Models**: I develop methods for precise control of diffusion-based generative models, enabling spatially-aware object manipulation that follows photorealistic principles and physical constraints. My research focuses on computation-aware architectures for diffusion models that maintain quality while improving efficiency.

5. **Agentic Systems**: I research autonomous AI systems that can plan, reason, and execute complex tasks through dynamic tool use and environment interaction. This includes developing architectures that support self-directed learning and decision-making in multi-step reasoning tasks.

The overarching goal of my research is to make ML systems more accessible, efficient, and deployable in real-world scenarios where computational resources are limited.

### Research Projects

#### Zero-Cost Neural Architecture Search
This project focuses on developing methods to evaluate the quality of neural architectures without expensive training. Our approach leverages network characteristics and topology to predict performance, making NAS more accessible and efficient.

#### Differentiable Pruning at Initialization
We explore novel techniques for pruning neural networks at initialization, allowing for significant model compression before training. Our DPaI method introduces a Node-Path Balance Principle that guides pruning decisions.

#### Generative Modeling for Neural Architecture Design
This work uses hierarchical generative models to create neural architectures, allowing for more flexible and controlled design of neural networks.

#### Computation-Aware Controllable Diffusion Models
This project focuses on developing efficient architectures for diffusion models that enable precise control over image generation and editing. We research methods to maintain high-quality output while significantly reducing computational requirements through differentiable routers and specialized control mechanisms.

#### Agentic Systems for Complex Reasoning Tasks
We research frameworks for autonomous AI agents that can plan, reason, and execute multi-step tasks. This work explores architectures that support dynamic tool use, environment interaction, and self-directed learning to solve complex problems that require sophisticated reasoning capabilities.

### Collaborations

I actively collaborate with several research teams:

* **Samsung AI Centre Cambridge**: Working on model compression and neural architecture search for efficient mobile AI.
* **University of Warwick**: Collaborating on various ML research projects and supervising doctoral students.
* **Collov.ai**: Leading research on 3D space reconstruction and diffusion models for interior design applications.

### Join or Collaborate

If you're interested in joining or collaborating on research related to efficient ML systems, neural architecture design, or model compression, please feel free to contact me at [l.xiang.2@warwick.ac.uk](mailto:l.xiang.2@warwick.ac.uk). 